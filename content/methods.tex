\section{Methods and Models}\label{sec:methods}
% RL method applied
% Experimental methods, (independent variables, dependent variables, control variables)
Waterworld has already been tackled by a number of authors.
Jonathan~Ho~et~al.~\cite{Ho2016} applied imitation learning (specifically, TRPO) to the
single-agent version of Waterworld using a unique model-free technique, and discovered
their technique showed promise.

Similarly, Jayesh~K.~Gupta~et~al.~\cite{Gupta2017} similarly applied a number of
reinforcement learning and imitation learning algorithms to the multi-agent version
of Waterworld and similar problems.
These algorithms include TRPO, A3C, and DDPG\@.
Each was applied using shared parameters, concurrent, and centralized techniques.
They discovered parameter-sharing A3C yielded the best results for Waterworld, while
TRPO performed best for centralized.
A3C was the only successful model for concurrent.

\subsection{Training Algorithms}\label{subsec:training-algorithms}
In this study, we apply three common reinforcement learning algorithms, as well as a
basic controls-agent hybrid, to train our agents.
Specifically, we use a continuous-space adapted Q-learning algorithm, as well as A2C,
and DDPG in addition to the hybrid.
We are not using TPRO because, as outlined by~\cite{Ho2016}, imitation learning tends
to have difficulty with complex environments and higher-dimensional states.
While the same authors outlined ways to overcome this obstacle, imitation learning
also relies on having some ``expert'' to show the agent how to act.
These experts are not always available, and so, in order to generalize our findings,
we will avoid using imitation learning except for the controls-agent hybrid.

\subsubsection{Q-learning}
The Q-learning algorithm works as a stacked Q-learning neural network (or QNN).
Each action dimension has its own policy QNN, which receive the same input.
Each policy has $n_\pi$ outputs, where $n >= 2$.
Since Q-learning works with probabilities for discrete actions, the continuous
action space over which the policy operates is divided into $n_\pi$ equal partitions,
with each output being associated with one partition.
For example, if the continuous actions space interval were $[-1, 1]$ (as it is for
Waterworld), and a policy has 5 outputs, the partitions would be $-1.0, -0.5, 0.0, 0.5,
\text{ and } 1.0$.
The partition with the highest associated output is selected as the action for that
dimension.
Essentially, the continuous space is converted to a discrete space that works with
the QNN\@.

\subsubsection{Advantage Actor-Critic (A2C)}
A2C, or Advantage Actor-Critic, works similarly to the Q-learning agent.
Each action dimension has an associated policy, and the policy outputs are used to
choose which partition of the discretized space to use.
The primary difference between the two is A2C uses a policy ``goodness'' or
``advantage'' network, called a \textit{critic}, to determine how good the current
and next states are.
The critic learns in parallel with the actor.
Additionally, both the policy and advantage networks can use a shared network to as
part of their computations.

This setup allows for the agent to determine its own advantage function, which
allows the system to more easily learn complex problems.

\subsubsection{Deep Deterministic Policy Gradient (DDPG)}
DDPG operates by having a policy model, called an \textit{actor}, and a ``goodness''
model, again called a \textit{critic}.
The actor is able to output continuous values, allowing for more fine-tuned control
than the discrete-space algorithms.
As with A2C, the critic determines the ``goodness'' of states, except in DDPG the
critic determines the ``goodness'' of an action for a state, giving it more control
over how the agent operates.
The critic also trains in parallel with the actor.

\subsubsection{Controls-Agent Hybrid}
The controls-agent hybrid is used to develop a critic model for DDPG\@.
It simply thrusts towards food, and away from poison if it cannot find any food.
This ``agent'' serves as both to show how much reward is possible, as well as train a
critic for DDPG\@.
The hope is to be able to create a strong critic before the DDPG actor starts
training, thereby jumpstarting the process.
The critic developed can then continue training with DDPG, or be locked.
Locking the critic will have the side effect that the actor will always be attempting
to mimic the controller algorithm instead of determining an alternative, higher value
solution.

\subsection{Model Architecture}\label{subsec:model-architecture}
% TODO

\subsection{Analysis Methods}\label{subsec:analysis-methods}
% TODO
